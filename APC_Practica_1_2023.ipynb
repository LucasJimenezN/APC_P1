{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZMnmdDBCl7F"
   },
   "source": [
    "# Pràctica 1: Resolem un problema de classificacio\n",
    "\n",
    "## Objectius\n",
    "\n",
    "Els objectius d'aquesta pràctica són:\n",
    "\n",
    "* Aplicar els coneixements adquirits sobre classificacio, tecniques d'emplenats de nans i validacio creuada.\n",
    "  \n",
    "* Ser capac de comparar diferents models classificacio\n",
    "\n",
    "* Ser capac de fer busca d'hiperparametres.\n",
    "\n",
    "* Entendre i implamentar la validacio creuada\n",
    "\n",
    "* Analitzar detalladament els resultats obtinguts durant l'aprenentatge dels diferents models.\n",
    "\n",
    "Aquesta practica es previa al cas kaggle que realitzareu durant la segona part de l'assignatura. En aquesta primera practica les preguntes estan definides, pero us ha de servir d'aprenentatge alhora de saber com estructurar un projecte d'aprenentatge automatic ja que en el cas kaggle no tindreu les preguntes.\n",
    "\n",
    "## Bases de dades\n",
    "\n",
    "En aquesta practica farem servir la base de dades del titanic. L'atribut que predirem es Survived, el qual ens diu si va sobreviure o no cada passatger.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "\n",
    "##Treball en grup\n",
    "Aquesta practica es treballara en grups de 2-3 persones. En casos excepcionals i degudament justificats la practica es podra realitzar de forma individual.\n",
    "## Seguiment i entrega de la pràctica\n",
    "\n",
    "En la pràctica 1, es presenten diverses tasques per fer una correcta comparativa dels resultats obtinguts per diversos mètodes de classificació numèrica en una mateixa base de dades.\n",
    "\n",
    "\n",
    "En aquesta practica es realitzara sessions de seguiment del treball. Aquestes sessions de treball està orientada a que els alumnes que vingueu pugueu preguntar i resoldre dubtes sobre les dades, preguntar sobre l'objectiu de cada apartat dels enunciats que no us hagi quedat clar, i preguntar sobre els resultats que esteu obtenint a l'hora d'analitzar les dades. Es molt recomanable venir amb el treball fet a clase per tal de poder comentar dubtes.\n",
    "\n",
    "Pel que fa l'entrega, caldra entregar per caronte el seguent:\n",
    "\n",
    "1. Memòria en format PDF o en la mateixa notebook explicant els resultats trobats sobre la bases de dades amb el respectiu codi de python. Aquesta nota es la mateixa per tots els membres del grup (9 pts)\n",
    "\n",
    "2. Presentació amb els resultats 4 min màxim. Aquesta nota es individual. (1 pt)\n",
    "\n",
    "Nota: En cas d'entregar la memoria com a PDF, caldra entregar el codi de python a part. Si s'entrega com a notebook, la mateixa notebook servira com a codi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zdgsxfuCl7S"
   },
   "source": [
    "# Descripcio de la pràctica\n",
    "\n",
    "A continuacio es mostren tots els continguts que s'evaluara en la pràctica:\n",
    "\n",
    "1. EDA (exploratory data analysis) (1 punts):\n",
    "  * Analisi de tamany i tipologia de dades ( 0.25 pt)\n",
    "  * Primera valoracio de correlacions ( 0.5 pt)\n",
    "  * Analisi atribut target ( 0.25 pt)\n",
    "2. Preprocessing (2 punts):\n",
    "  * Eliminacio de nans (0.5 punts)\n",
    "  * Encoding de categoriques (0.75 punts)\n",
    "  * Altres (PCA, normalitzacio..)\n",
    "3. Metric selection (1.5 punts):\n",
    "  * Seleccio de la millor metrica pel problema (0.75 punts)\n",
    "  * Visualitzacio de ROC/AUC (0.75 punts)\n",
    "4. Model Selection amb Crossvalidation (4 punts):\n",
    "  * Seleccio del millor model (2 punts)\n",
    "  * Busqueda hiperparametres (2 punts)\n",
    "5. Analisi final (1.5 punt)\n",
    "\n",
    "Cal dir que, les puntuacions dins de cada apartat son orientatives. La pràctica esta construida a partir d'un seguit de preguntes orientatives en cada apartat les quals tenen relacio amb els continguts evaluables. **NO cal contestar-les totes**. Són una guia per a que reflexioneu i aprengueu detalls de cada apartat.  Es recomanable, aixo si, llegir totes les preguntes abans de realitzar la practica i tenir-les en ment alhora d'executar-la.\n",
    "\n",
    "\n",
    "**IMPORTANT**: El que es valorara en la practica es la capacitat de mantenir una narrativa coherent alhora que es realitzen els resultats. No es mirara tant que alguna pregunta quedi per respondre sino que els passos seguits en base als resultats obtinguts siguin coherents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.723761Z",
     "start_time": "2023-11-04T14:59:39.626525Z"
    }
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuENGdYFCl7S"
   },
   "source": [
    "### 1. EDA (exploratory data analysis) (1 punt)\n",
    "\n",
    "Abans de res cal sempre veure com es la base de dades asignada?\n",
    "\n",
    "**Preguntes:**\n",
    "* Quants atributs té la vostra base de dades?\n",
    "* Quin tipus d'atributs tens? (Númerics, temporals, categorics, binaris...)\n",
    "* Com es el target, quantes categories diferents existeixen?\n",
    "* Tenim nans en les dades?\n",
    "* Podeu veure alguna correlació entre X i y?\n",
    "* Estan balancejades les etiquetes (distribució similar entre categories)? Creus que pot afectar a la classificació la seva distribució?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anàlisi de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.732431Z",
     "start_time": "2023-11-04T14:59:39.722263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La nostra base de dades té 12 atributs.\n"
     ]
    }
   ],
   "source": [
    "# Llegim la base de dades\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Obtenim el numero d'atributs\n",
    "num_atr = df.shape[1]\n",
    "\n",
    "# Veiem quants atributs te el nostre csv\n",
    "print(f\"La nostra base de dades té {num_atr} atributs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem doncs que la nostra base de dades disposa de 12 atributs, els quals tenen els següents noms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.735814Z",
     "start_time": "2023-11-04T14:59:39.731784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els noms dels atributs son: \n",
      "PassengerId\n",
      "Survived\n",
      "Pclass\n",
      "Name\n",
      "Sex\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Ticket\n",
      "Fare\n",
      "Cabin\n",
      "Embarked\n"
     ]
    }
   ],
   "source": [
    "atributos = df.columns.tolist()\n",
    "\n",
    "print(f\"Els noms dels atributs son: \")\n",
    "for nombre in atributos:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara ens interesa saber el tipus de dades que guarden aquests atributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.744007Z",
     "start_time": "2023-11-04T14:59:39.737708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Els tipus de dades dels atributs son:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object.\n"
     ]
    }
   ],
   "source": [
    "atr_type = df.dtypes\n",
    "\n",
    "print(f\"Els tipus de dades dels atributs son:\\n{atr_type}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara ens interesa saber com es el target i quantes categories diferents existeixen:\n",
    "Despres de veure la base de dades, podem deduir que el nostre target a analitzar és la columna \"Survived\", la qual ens indica si el passatger va sobreviure (1) o no (0), veiem també que les diferents categories possibles(2) son 0, no sobreviu, 1 sobreviu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.794457Z",
     "start_time": "2023-11-04T14:59:39.744986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de categories que te l'atribut 'Survived' és: 2\n"
     ]
    }
   ],
   "source": [
    "num_categories_survived = df['Survived'].nunique()\n",
    "\n",
    "print(f\"El numero de categories que te l'atribut 'Survived' és: {num_categories_survived}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executant el codi python anterior, veiem que la nostra deducció de possibles valors categorics es correcte, i son 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara hem de determinar si la nostra base de dades te o no nans, això ho fariem de la següent manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.794955Z",
     "start_time": "2023-11-04T14:59:39.750987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de dades té nans.\n"
     ]
    }
   ],
   "source": [
    "df_nans = df.isna().any().any()\n",
    "\n",
    "if df_nans:\n",
    "    print(f\"La base de dades té nans.\")\n",
    "else:\n",
    "    print(f\"La base de dades no té nans.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara hem de veure si hi ha alguna corelació entre X i Y a la hora de determinar l'atribut \"Survived\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.795740Z",
     "start_time": "2023-11-04T14:59:39.760634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replicamos la base de datos eliminando las columnas que no nos interesan.\n",
    "dataset_corr = df.copy()\n",
    "useless_columns = []\n",
    "\n",
    "for f in dataset_corr:\n",
    "    if f == 'Name' or f == 'Ticket' or f == 'Cabin':\n",
    "        useless_columns.append(f)\n",
    "\n",
    "dataset_corr.drop(useless_columns, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas Name , Ticket y Cabin por los motivos que especificamos a continuación:\n",
    "- Name: La eliminamos puesto que es un indicador del nombre del pasajero y no es relevante.\n",
    "- Ticket: La eliminamos puesto que es un indicador del ticket del pasajero y no es relevante.\n",
    "- Cabin: La eliminamos puesto que es un indicador de la cabina de cada pasajero y tampoco es relevante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.796239Z",
     "start_time": "2023-11-04T14:59:39.765456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos los valores de sex a 0 y 1. ( codificación one-hot )\n",
    "df_encoded = pd.get_dummies(dataset_corr, columns=['Sex'], drop_first=True)\n",
    "\n",
    "# Convertimos los valores de Embarked.\n",
    "#maping = {'C' : 0 , 'Q': 1, 'S' : 2}\n",
    "#df_encoded['Embarked_numeric'] = df_encoded['Embarked'].map(maping)\n",
    "#df_encoded = df_encoded.drop(columns=['Embarked'])\n",
    "\n",
    "# enc = preprocessing.OrdinalEncoder()\n",
    "# encoded_data = enc.fit_transform(df_encoded['Embarked'])\n",
    "# df_encoded['Embarked'] = encoded_data\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['Embarked'], drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo de arriba en primer lugar, eliminar la columna 'Sex', creando una llamada 'Sex_male', la cual es 1 si el usuario es hombre y 0 si mujer, de esta manera podremos calcular la correlación con 'Survived'. En segundo lugar, hemos eliminado la columna 'Embarked', creando la columna 'Embarked_numeric', que sustituie los valores C, Q y S, por 0, 1, 2 respectivamente, consiguiendo así poder calcular la correlación.\n",
    "A continuación se muestra como queda la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.796941Z",
     "start_time": "2023-11-04T14:59:39.772185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  \\\n",
      "0              1         0       3  22.0      1      0   7.2500      True   \n",
      "1              2         1       1  38.0      1      0  71.2833     False   \n",
      "2              3         1       3  26.0      0      0   7.9250     False   \n",
      "3              4         1       1  35.0      1      0  53.1000     False   \n",
      "4              5         0       3  35.0      0      0   8.0500      True   \n",
      "..           ...       ...     ...   ...    ...    ...      ...       ...   \n",
      "886          887         0       2  27.0      0      0  13.0000      True   \n",
      "887          888         1       1  19.0      0      0  30.0000     False   \n",
      "888          889         0       3   NaN      1      2  23.4500     False   \n",
      "889          890         1       1  26.0      0      0  30.0000      True   \n",
      "890          891         0       3  32.0      0      0   7.7500      True   \n",
      "\n",
      "     Embarked_C  Embarked_Q  Embarked_S  \n",
      "0         False       False        True  \n",
      "1          True       False       False  \n",
      "2         False       False        True  \n",
      "3         False       False        True  \n",
      "4         False       False        True  \n",
      "..          ...         ...         ...  \n",
      "886       False       False        True  \n",
      "887       False       False        True  \n",
      "888       False       False        True  \n",
      "889        True       False       False  \n",
      "890       False        True       False  \n",
      "\n",
      "[891 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos hecho las modificaciones pertinentes a nuesta base de datos, ya podemos caluclar la correlación con la variable 'Survived'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.810266Z",
     "start_time": "2023-11-04T14:59:39.778440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pclass': -0.3384810359610148, 'Age': -0.07722109457217755, 'SibSp': -0.035322498885735576, 'Parch': 0.0816294070834836, 'Fare': 0.2573065223849622, 'Sex_male': -0.5433513806577547, 'Embarked_C': 0.16824043121823296, 'Embarked_Q': 0.0036503826839719864, 'Embarked_S': -0.15566027340439323}\n"
     ]
    }
   ],
   "source": [
    "new_names = df_encoded.columns.to_list()\n",
    "\n",
    "corr = {}\n",
    "for name in new_names: \n",
    "    if name == 'Survived' or name == 'PassengerId':\n",
    "        pass\n",
    "    else:\n",
    "        corr_value = df_encoded['Survived'].corr(df_encoded[name])\n",
    "        corr[name] = corr_value\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los resultados pasamos a analizarlos:\n",
    "- Pclass: -0,338\n",
    "    - Esta correlación es negativa y moderadamente fuerte. Indica que a medida de que Pclass disminuye ( lo que significa que el pasajero está en una clase mas alta ) la probabilidad de sobrevivir aumenta.\n",
    "- Age: -0,077\n",
    "    - Esta correlación es negativa y bastante baja. Lo que indica que a medida que la edad del pasajero aumenta, la probabilidad de sobrebirir del pasajero disminuye ligeramente. Pero como lo correlación es bastante baja podemos deducir que este no es un factor determinante.\n",
    "- SibSp: -0.035\n",
    "    - Esta correlación también es negativa y bastante baja. Indica que a medida que el numero de hermanos o cónyuges a bordo aumenta, la probabilidad de sobrevivir disminuye ligeramente. Como la correlación es bastante baja podemos deducir que no es un factor determinante.\n",
    "- Parch: 0,082\n",
    "    - Esta correlación es positiva y bastante baja. Indica que a medida que el numero padres o hijos a bordo aumenta, la posibilidad de sobrevivir aumenta ligeramnete. Como la correlación es bastante baja podemos deducir que no es un factor determinante.\n",
    "- Fare: 0.257\n",
    "    - Esta correlación es positiva y relativamente fuerte. Indica que a medida que el coste del billete aumenta, la posibilidad de sobrevivir aumente moderadamente. Como esta correlación es moderada podemos deducir que puede ser un factor determinante, puesto que los pasageros que havian pagado mas, quizas tenian prioridad a la hora de ser rescatados.\n",
    "- Sex_male: -0,543\n",
    "    - Esta correlación es negativa y fuerte. Indica que a ser del sexo masculino, disminuia drasticamente las posibilidades de sobrevivir, por lo que, las mujeres, tenian prioridad a la hora del rescate. Esta correlación al ser bastante fuerte podemos decir que es un factor determinante de la supervivencia del pasagero.\n",
    "- Embarked_C: 0,168\n",
    "    - Esta correlación es positiva y moderadamente baja. Indica que el puerto de embarque C está asociado a la probabilidad de sobrevivir, pero al ser moderadamente baja, no es un factor determinante.\n",
    "- Embarked_Q: 0,00365\n",
    "    -  Esta correlación es positiva y muy baja. Indica que el puerto de embarque Q no está asociado a la probabilidad de sobrevivir. \n",
    "-  Embarked_S: -0,1557\n",
    "    -  Esta correlación es negativa y moderadamente baja. Indica que el puerto de embarque S está asociado a la probabilidad de sobrevivir, pero al ser baja, no es un factor determinante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos analizado la correlación, necesitamos saber si las etiquetas están balanceadas y si puede afectar a la clasificación de su distribución.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.810797Z",
     "start_time": "2023-11-04T14:59:39.786478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "survived_count = df_encoded['Survived'].value_counts()\n",
    "total_inst = len(df_encoded)\n",
    "\n",
    "print(survived_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la ejecución previa, podemos ver que no hay una diferencia muy grande entre los pasajeros que sobrevivieron y los que no, aunque eso si, hay mas pasajeros que no sobrevivieron. \n",
    "Podemos deducir que la deducción de las etiquetas puede afectar a su clasificación. Puesto que, en nuestro caso como favorecen las instancias de que los pasajeros no sobreviven nuesto modelo puede empezar a favorecer esta clase y no ser tan precisos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuLtWQWpCl7T"
   },
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection..) (2 punts)\n",
    "Un cop vistes les dades de les que es disposa, cal preparar les dades per als nostres algoritmes. Segons la tipologia de dades, es poden filtrar atributs, aplicar-hi reductors de dimensionalitat, codificar categories textuals en valors numèrics, normalitzar les dades, treure outliers...\n",
    "\n",
    "Navegueu per la [documentació de sklearn sobre preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) per tal de trobar les diferents opcions que proporciona sklearn.\n",
    "\n",
    "**Preguntes:**\n",
    "* Estàn les dades normalitzades? Caldria fer-ho?\n",
    "* En cas que les normalitzeu, quin tipus de normalització será més adient per les vostres dades?\n",
    "* Teniu gaires dades sense informació (nans)? Tingueu en compte que hi ha metodes que no els toleren durant el aprenentatge. Com afecta a la classificació si les filtrem? I si les reompliu? Com ho farieu? [Pista](https://scikit-learn.org/stable/modules/impute.html)\n",
    "* Teniu dades categoriques? Quina seria la codificació amb més sentit?\n",
    "* Podreu treure algun atribut extra de les categoriques (per exemple, aplicant alguna regla sobre el text)?\n",
    "* Caldria aplicar PCA? Quins beneficis o inconvenients trobarieu?\n",
    "* Caldria aplicar alguna tecnica de seleccio de features? Ho trobeu necessari?\n",
    "* Es poden aplicar PolynomialFeatures per millorar la classificació? En quins models té sentit fer-ho?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 1\n",
    "¿Están los datos normalizados? ¿Tendríamos que hacerlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestra base de datos original, los datos no están normalizados, para que los datos estén normalizados, todas las características deberían estar en una escala similar.\n",
    "Esto sirve ya que en nuestro modelo puede tomar como referencia los valores mas altos de diferentes categorias y de esta manera entrenarse de manera erronea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.810928Z",
     "start_time": "2023-11-04T14:59:39.788997Z"
    }
   },
   "outputs": [],
   "source": [
    "### Codigo de normalización ###\n",
    "#df_normal = preprocessing.normalize(df_encoded, norm=\"l1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código lo que hace es normalizar nuestra base de datos, peró no se está ejecutando correctamente puesto que nuestra base de datos contiene nans, por lo que a continuación vamos a gestionar los nans y luego normalizaremos nuestra base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 2\n",
    "¿En caso de normalizarlas, qué tipo de normalización tenemos/hemos usado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 3\n",
    "¿Hay varios datos sin valores(nans)? Como afecta la clasificación si los filtramos? Como lo hariamos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver si nuestra base de datos modificada tiene nans ejecutariamos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.813184Z",
     "start_time": "2023-11-04T14:59:39.793226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "df_encoded_nans = df_encoded.isna().sum().sum()\n",
    "print(df_encoded_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que si tenemos nans en nuestra tabla por lo que ahora vamos a analizarlos des del punto de vista de las filas y de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:39.826526Z",
     "start_time": "2023-11-04T14:59:39.797696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.000000\n",
      "1      0.000000\n",
      "2      0.000000\n",
      "3      0.000000\n",
      "4      0.000000\n",
      "         ...   \n",
      "886    0.000000\n",
      "887    0.000000\n",
      "888    0.090909\n",
      "889    0.000000\n",
      "890    0.000000\n",
      "Length: 891, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Nans por porcentage de filas.\n",
    "percent_nan_row = df_encoded.isna().mean(axis=1)\n",
    "print(percent_nan_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:00:19.195843Z",
     "start_time": "2023-11-04T15:00:19.111882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGbCAYAAAD0h4tNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkIUlEQVR4nO3df1Sb9f338VdoqERa74Tq9HiO53i2BJ2KC1Ir6ETXfWPPDqVUCv44tcd2m9Xq/HUsumm1PTIs2079wdn0OJSDm7gqHDmK7RR3jlPXFaTKqLqhROec9qwV2iIF0qZw3X/0hrv50louvMInyZ6Pc3p2mutK8vnkTcxzaQCXZVmWAAAADEozvQAAAACCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGOc2vQC7+voG5PQPu3e5pDlzZsfltmEPs0gczCJxMIvEwSzsG3vMjiXpgsSyFLcvgnjeNuxhFomDWSQOZpE4mIXz+CcbAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYl3W/7jacZM5Krz0ZHLY2O8usmAQDJjyCRlJbm0sioJZ8v0/RSbDk4Mqr+vUNECQAg6REkklwul2akuXTrxk6Fd+0zvZxJ8X9jlh65KldpaS6CBACQ9AiSw4R37dP7O740vQwAAP7r2PrQxIsvvqjc3NyYP+ecc47OOeccSVJXV5fKy8uVm5ur+fPnq7GxMeb6zc3NCoVCCgaDKi0tVWdnp3M7AQAASctWkCxatEidnZ3jf15++WV5vV5VVVWpv79fK1eu1OLFi9XR0aGqqiqtX79e27dvlyS1t7ersrJS1dXV6ujo0KJFi7Rq1SoNDw/HZWMAACB5TPnbSizLUkVFhS699FKVlJSotbVVXq9XS5culdvtVkFBgYqLi9XQ0CBJamxsVFFRkfLy8pSenq7ly5fL5/Np8+bNjm0GAAAkpyl/huSFF15QOBzWo48+Kknq6elRdnZ2zDl+v19NTU2SpHA4rCVLlkw43t3dbet+Xa6prnh6b3M6Jfv6Dze2l1TaU7JiFomDWSQOZmHfZB+rKQXJ6OioHnvsMd1www2aNWuWJGlwcFAejyfmvIyMDA0NDU3q+GTNmTN7KktOWcn2rcqTxZwTB7NIHMwicTAL500pSNrb27Vr1y6VlZWNX+bxeDQwMBBzXiQSUWZm5vjxSCQy4bjP57N13319A7Ic/i5XtztNXm9yvrDv2TOokZFR08twjMt16IkejznDHmaROJhF4mAW9o09ZscypSB55ZVXFAqFdPzxx49flp2drS1btsScFw6HFQgEJEmBQEA9PT0TjhcWFtq6b8uS418Eyf5FlezrP5J4zBlTwywSB7NIHMzCeVP6UOvbb7+t888/P+ayUCik3t5e1dfXKxqNqq2tTS0tLeOfGykrK1NLS4va2toUjUZVX1+vvr4+hUKhr78LAACQ1Kb0Dslnn32mb3zjGzGX+Xw+1dXVqaqqSjU1NcrKytKaNWuUn58vSSooKNDatWu1bt067dy5U36/X7W1tfJ6vV97EwAAILlNKUiO9gPNcnJytHHjxqNer6SkRCUlJVO5SwAAkMKS69fbAgCAlESQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMbZDpK9e/fqzjvv1AUXXKDzzz9fN954o3bt2iVJ6urqUnl5uXJzczV//nw1NjbGXLe5uVmhUEjBYFClpaXq7Ox0ZhcAACCp2Q6Sm2++WUNDQ3r11Vf12muvacaMGbr33nvV39+vlStXavHixero6FBVVZXWr1+v7du3S5La29tVWVmp6upqdXR0aNGiRVq1apWGh4cd3xQAAEgutoLkvffeU1dXl6qrq3XCCSdo1qxZqqys1OrVq9Xa2iqv16ulS5fK7XaroKBAxcXFamhokCQ1NjaqqKhIeXl5Sk9P1/Lly+Xz+bR58+a4bAwAACQPt52Tt2/fLr/fr+eee05/+MMfNDw8rIsvvlh33XWXenp6lJ2dHXO+3+9XU1OTJCkcDmvJkiUTjnd3d9tasMtl63Rjtzmdkn39hxvbSyrtKVkxi8TBLBIHs7Bvso+VrSDp7+/XBx98oHPOOUfNzc2KRCK68847ddddd+nEE0+Ux+OJOT8jI0NDQ0OSpMHBwa88Pllz5sy2dX6q8/kyTS8hLphz4mAWiYNZJA5m4TxbQTJz5kxJ0j333KPjjjtOs2bN0m233aYrrrhCpaWlikQiMedHIhFlZh56wfR4PEc87vP5bC24r29AlmXrKsfkdqfJ603OF/Y9ewY1MjJqehmOcbkOPdHjMWfYwywSB7NIHMzCvrHH7FhsBYnf79fo6Kii0aiOO+44SdLo6KEXw29/+9t65plnYs4Ph8MKBAKSpEAgoJ6engnHCwsL7SxBliXHvwiS/Ysq2dd/JPGYM6aGWSQOZpE4mIXzbH2o9cILL9Rpp52mu+++W4ODg9q9e7ceeugh/c///I8WLlyo3t5e1dfXKxqNqq2tTS0tLeOfGykrK1NLS4va2toUjUZVX1+vvr4+hUKhuGwMAAAkD1tBkp6ert///veaMWOGFixYoAULFuiUU07RAw88IJ/Pp7q6Or388su64IILtGbNGq1Zs0b5+fmSpIKCAq1du1br1q3TvHnztGnTJtXW1srr9cZjXwAAIInY+icbSTr55JP10EMPHfFYTk6ONm7ceNTrlpSUqKSkxO5dAgCAFMePjgcAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADDOdpBs3rxZZ511lnJzc8f/VFRUSJK6urpUXl6u3NxczZ8/X42NjTHXbW5uVigUUjAYVGlpqTo7O53ZBQAASGpuu1d49913VVJSovXr18dc3t/fr5UrV+qWW27RlVdeqY6ODt10000644wzdO6556q9vV2VlZWqra3Vueeeq4aGBq1atUqvvfaaPB6PYxsCAADJx/Y7JO+++67OOeecCZe3trbK6/Vq6dKlcrvdKigoUHFxsRoaGiRJjY2NKioqUl5entLT07V8+XL5fD5t3rz56+8CAAAkNVvvkIyOjur999+Xx+PRE088oZGREV1yySVavXq1enp6lJ2dHXO+3+9XU1OTJCkcDmvJkiUTjnd3d9tasMtl63Rjtzmdkn39hxvbSyrtKVkxi8TBLBIHs7Bvso+VrSDZvXu3zjrrLC1YsEA1NTXas2eP7rrrLlVUVOikk06a8E8vGRkZGhoakiQNDg5+5fHJmjNntq3zU53Pl2l6CXHBnBMHs0gczCJxMAvn2QqSE088cfyfYCTJ4/GooqJCV1xxhUpLSxWJRGLOj0QiyszMHD/3SMd9Pp+tBff1DciybF3lmNzuNHm9yfnCvmfPoEZGRk0vwzEu16EnejzmDHuYReJgFomDWdg39pgdi60g6e7u1ksvvaQ77rhDrv/3HsyBAweUlpamc889V0899VTM+eFwWIFAQJIUCATU09Mz4XhhYaGdJciy5PgXQbJ/USX7+o8kHnPG1DCLxMEsEgezcJ6tD7V6vV41NDToiSee0MGDB7Vjxw796le/0uWXX64FCxaot7dX9fX1ikajamtrU0tLy/jnRsrKytTS0qK2tjZFo1HV19err69PoVAoLhsDAADJw9Y7JKeccooef/xxPfjgg3rsscd03HHHqaioSBUVFTruuONUV1enqqoq1dTUKCsrS2vWrFF+fr4kqaCgQGvXrtW6deu0c+dO+f1+1dbWyuv1xmNfAAAgidj+OSTz5s3Txo0bj3gsJyfnqMckqaSkRCUlJXbvEgAApDh+dDwAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMm1KQjIyMaNmyZfrpT386fllXV5fKy8uVm5ur+fPnq7GxMeY6zc3NCoVCCgaDKi0tVWdn59dbOQAASBlTCpJf//rX2rZt2/jf+/v7tXLlSi1evFgdHR2qqqrS+vXrtX37dklSe3u7KisrVV1drY6ODi1atEirVq3S8PCwM7sAAABJzXaQbN26Va2trbrsssvGL2ttbZXX69XSpUvldrtVUFCg4uJiNTQ0SJIaGxtVVFSkvLw8paena/ny5fL5fNq8ebNzOwEAAEnLbefkvr4+3XPPPXr00UdVX18/fnlPT4+ys7NjzvX7/WpqapIkhcNhLVmyZMLx7u5u2wt2uWxfxchtTqdkX//hxvaSSntKVswicTCLxMEs7JvsYzXpIBkdHVVFRYVWrFihM888M+bY4OCgPB5PzGUZGRkaGhqa1HE75syZbfs6qcznyzS9hLhgzomDWSQOZpE4mIXzJh0kjz/+uGbOnKlly5ZNOObxeDQwMBBzWSQSUWZm5vjxSCQy4bjP57O94L6+AVmW7at9Jbc7TV5vcr6w79kzqJGRUdPLcIzLdeiJHo85wx5mkTiYReJgFvaNPWbHMukgeeGFF7Rr1y7NnTtXksYD409/+pPuvPNObdmyJeb8cDisQCAgSQoEAurp6ZlwvLCwcLJ3P86y5PgXQbJ/USX7+o8kHnPG1DCLxMEsEgezcN6kP9T68ssv65133tG2bdu0bds2LVy4UAsXLtS2bdsUCoXU29ur+vp6RaNRtbW1qaWlZfxzI2VlZWppaVFbW5ui0ajq6+vV19enUCgUt40BAIDkYetDrUfj8/lUV1enqqoq1dTUKCsrS2vWrFF+fr4kqaCgQGvXrtW6deu0c+dO+f1+1dbWyuv1OnH3AAAgyU05SKqrq2P+npOTo40bNx71/JKSEpWUlEz17gAAQArjR8cDAADjCBIAAGAcQQIAAIwjSAAAgHEECQAAMI4gAQAAxhEkAADAOIIEAAAYR5AAAADjCBIAAGAcQQIAAIwjSAAAgHEECQAAMI4gAQAAxhEkAADAOIIEAAAYR5AAAADjCBIAAGAcQQIAAIwjSAAAgHEECQAAMI4gAQAAxhEkAADAOIIEAAAYR5AAAADjCBIAAGAcQQIAAIwjSAAAgHEECQAAMI4gAQAAxhEkAADAOIIEAAAYR5AAAADjCBIAAGAcQQIAAIwjSAAAgHEECQAAMI4gAQAAxhEkAADAONtBsnXrVpWXl+u8887TRRddpMrKSkUiEUlSV1eXysvLlZubq/nz56uxsTHmus3NzQqFQgoGgyotLVVnZ6czuwAAAEnNVpDs3r1b119/va6++mpt27ZNzc3Neuutt/Tb3/5W/f39WrlypRYvXqyOjg5VVVVp/fr12r59uySpvb1dlZWVqq6uVkdHhxYtWqRVq1ZpeHg4LhsDAADJw1aQZGVl6a9//atKS0vlcrm0d+9e7d+/X1lZWWptbZXX69XSpUvldrtVUFCg4uJiNTQ0SJIaGxtVVFSkvLw8paena/ny5fL5fNq8eXNcNgYAAJKH2+4VZs2aJUm65JJLtHPnTs2dO1elpaV6+OGHlZ2dHXOu3+9XU1OTJCkcDmvJkiUTjnd3d9u6f5fL7orN3OZ0Svb1H25sL6m0p2TFLBIHs0gczMK+yT5WtoNkTGtrq/r7+7V69WrdcsstOvnkk+XxeGLOycjI0NDQkCRpcHDwK49P1pw5s6e65JTk82WaXkJcMOfEwSwSB7NIHMzCeVMOkoyMDGVkZKiiokLl5eVatmyZBgYGYs6JRCLKzDz0gunxeMY//Hr4cZ/PZ+t++/oGZFlTXfWRud1p8nqT84V9z55BjYyMml6GY1yuQ0/0eMwZ9jCLxMEsEgezsG/sMTsWW0Hyzjvv6O6779aLL76omTNnSpIOHDig9PR0+f1+bdmyJeb8cDisQCAgSQoEAurp6ZlwvLCw0M4SZFly/Isg2b+okn39RxKPOWNqmEXiYBaJg1k4z9aHWs844wxFIhFt2LBBBw4c0Oeff65f/OIXKisr04IFC9Tb26v6+npFo1G1tbWppaVl/HMjZWVlamlpUVtbm6LRqOrr69XX16dQKBSXjQEAgORh6x2SzMxMPfHEE3rggQd00UUXafbs2SouLtZNN92kmTNnqq6uTlVVVaqpqVFWVpbWrFmj/Px8SVJBQYHWrl2rdevWaefOnfL7/aqtrZXX643HvgAAQBKx/RkSv9+vurq6Ix7LycnRxo0bj3rdkpISlZSU2L1LAACQ4vjR8QAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMbZCpLu7m6tWLFC8+bN00UXXaQ777xTu3fvliR1dXWpvLxcubm5mj9/vhobG2Ou29zcrFAopGAwqNLSUnV2djq3CwAAkNQmHSSRSEQ//vGPlZubq7/85S966aWXtHfvXt19993q7+/XypUrtXjxYnV0dKiqqkrr16/X9u3bJUnt7e2qrKxUdXW1Ojo6tGjRIq1atUrDw8Nx2xgAAEgekw6SHTt26Mwzz9RNN92kmTNnyufz6corr1RHR4daW1vl9Xq1dOlSud1uFRQUqLi4WA0NDZKkxsZGFRUVKS8vT+np6Vq+fLl8Pp82b94ct40BAIDk4Z7sid/85jf1xBNPxFz2yiuv6Oyzz1ZPT4+ys7Njjvn9fjU1NUmSwuGwlixZMuF4d3e37QW7XLavYuQ2p1Oyr/9wY3tJpT0lK2aROJhF4mAW9k32sZp0kBzOsiw9/PDDeu211/T000/rd7/7nTweT8w5GRkZGhoakiQNDg5+5XE75syZPZUlpyyfL9P0EuKCOScOZpE4mEXiYBbOsx0k+/bt089+9jO9//77evrpp3XGGWfI4/FoYGAg5rxIJKLMzEMvlh6PR5FIZMJxn89ne8F9fQOyLNtX+0pud5q83uR8Yd+zZ1AjI6Oml+EYl+vQEz0ec4Y9zCJxMIvEwSzsG3vMjsVWkHz66ae67rrrdOqpp6qpqUlZWVmSpOzsbG3ZsiXm3HA4rEAgIEkKBALq6emZcLywsNDO3UuSLEuOfxEk+xdVsq//SOIxZ0wNs0gczCJxMAvnTfpDrf39/br22mt13nnn6cknnxyPEUkKhULq7e1VfX29otGo2tra1NLSMv65kbKyMrW0tKitrU3RaFT19fXq6+tTKBRyfkcAACDpTPodkueff147duzQH//4R7388ssxxzo7O1VXV6eqqirV1NQoKytLa9asUX5+viSpoKBAa9eu1bp167Rz5075/X7V1tbK6/U6uhkAAJCcXJaVXG869fbG5zMkPl+mimre1Ps7vnT2xuPk7FNP0KZbLtaePYM6eDC1PkNy4omz4zJn2MMsEgezSBzMwr6xx+xY+NHxAADAOIIEAAAYR5AAAADjCBIAAGDclH5SKwAAOLq0NJfS0pLr58uPjloaHTX3SV2CBAAAB6WlufR/vMfLPSO5/hHi4Mio+vcOGYsSggQAAAelpbnknpGmWzd2Krxrn+nlTIr/G7P0yFW5SktzESQAAKSS8K59SfOzrRJBcr2fBAAAUhJBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABg35SDZvXu3QqGQ2tvbxy/r6upSeXm5cnNzNX/+fDU2NsZcp7m5WaFQSMFgUKWlpers7Jz6ygEAQMqYUpC8/fbbuvLKK/Xpp5+OX9bf36+VK1dq8eLF6ujoUFVVldavX6/t27dLktrb21VZWanq6mp1dHRo0aJFWrVqlYaHh53ZCQAASFq2g6S5uVmrV6/W7bffHnN5a2urvF6vli5dKrfbrYKCAhUXF6uhoUGS1NjYqKKiIuXl5Sk9PV3Lly+Xz+fT5s2bndkJAABIWm67V/jud7+r4uJiud3umCjp6elRdnZ2zLl+v19NTU2SpHA4rCVLlkw43t3dbev+XS67KzZzm9Mp2dd/uLG9pNKekhWzSBzMInH8N8zC6b1N9vZsB8lJJ510xMsHBwfl8XhiLsvIyNDQ0NCkjk/WnDmzbZ2f6ny+TNNLiAvmnDiYReJgFokjVWdh8jXFdpAcjcfj0cDAQMxlkUhEmZmZ48cjkciE4z6fz9b99PUNyLK+3lr/N7c7TV5vcr6w79kzqJGRUdPLcIzLdeiJHo85wx5mkTiYReKYzCxmzEhL2v+zGI/XlLHH7FgcC5Ls7Gxt2bIl5rJwOKxAICBJCgQC6unpmXC8sLDQ1v1Ylhx/Qib7EzzZ138k8ZgzpoZZJA5mkThSeRam9uXYzyEJhULq7e1VfX29otGo2tra1NLSMv65kbKyMrW0tKitrU3RaFT19fXq6+tTKBRyagkAACBJOfYOic/nU11dnaqqqlRTU6OsrCytWbNG+fn5kqSCggKtXbtW69at086dO+X3+1VbWyuv1+vUEgAAQJL6WkHywQcfxPw9JydHGzduPOr5JSUlKikp+Tp3CQAAUhA/Oh4AABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGESQAAMA4ggQAABhHkAAAAOMIEgAAYBxBAgAAjCNIAACAcQQJAAAwjiABAADGTWuQ9PX16cYbb9TcuXN1wQUXqKqqSgcPHpzOJQAAgAQ0rUFy22236fjjj9ebb76ppqYmbd26VfX19dO5BAAAkICmLUj+9a9/6a233lJFRYU8Ho9OO+003XjjjWpoaJiuJQAAgATlnq476unpkdfr1cknnzx+2be+9S3t2LFDX375pU444YRJ3U5ammRZzq7N5Tr0v2efeoI8M2c4e+Nx8s0TMyVJM2Yk18eALOv/P95HMnbM7U5zfM5Tdaw1JyIn1jzds/hvfZwnw8lZ8Dh/PZOZxdh/l5PxNUU69DrrpMnObtqCZHBwUB6PJ+aysb8PDQ1NOkiysmY7vrYxvyz7TtxuO15OOMFz7JOSkNebeeyTMC2YReJgFoljMrNIxtcUn8/c19i0/d/r448/XsPDwzGXjf09M5MnGQAA/82mLUgCgYD27t2r3t7e8cs++ugjnXLKKZo9O37vegAAgMQ3bUFy+umnKy8vTw888ID27dunf//733r00UdVVlY2XUsAAAAJymVZ0/fRwd7eXt1///1qb29XWlqaFi9erNWrV2vGjOT40A8AAIiPaQ0SAACAI0mu7xkFAAApiSABAADGESQAAMA4ggQAABiXskFi5zcLv/766youLlYwGNQPfvADvfbaazHHa2trVVhYqGAwqGXLlunjjz+eji2kDKdmsX//flVVVamwsFB5eXkqLy9XW1vbdG0jJTj5vBjT2NioM844I57LTklOzuKZZ55RKBRSbm6uiouLjzorHJlTs4hEIrrvvvt00UUX6fzzz9e1116r7u7u6dpG8rNS1DXXXGPdcccd1tDQkPXpp59aRUVFVm1t7YTz/vnPf1o5OTnWq6++akWjUWvTpk3Wueeea/3nP/+xLMuynn/+eeviiy+2PvzwQysSiVjr16+3ioqKrNHR0eneUtJyahY///nPrdLSUmvHjh3WwYMHrWeffdb6zne+Y33++efTvaWk5dQsxnz44YdWMBi0srOzp2sLKcPJ/0ZdeOGFVldXlzU6Omq1tLRYZ5999oRZ4eicmsUvf/lLa9myZdaePXus/fv3Ww888ID1/e9/f7q3k7RSMkg++eQTKzs7O+YJuWnTJuvSSy+dcO6DDz5orVixIuayH/3oR9YjjzxiWZZlXXXVVdZjjz02fuzAgQNWbm6utXXr1jitPrU4OYt7773X+vOf/xxz/Pzzz7daW1vjsPLU4+QsLMuyhoaGrIULF1oPPvggQWKTk7NYuHCh9eyzz8Ycf++996x9+/bFYeWpx8lZXH/99dY111xj7d6929q/f79VXV1tLVy4ML4bSCEp+U82x/rNwocLh8PKzs6Ouczv94+/zfa/j6enp+v000/nbbhJcnIW999/vy655JLxY1u3btXAwIDOPPPMOO4gdTg5C+nQPC699FJdeOGF8V14CnJqFsPDw+rp6VFaWpqWLl2qCy64QFdddZWGh4f5HWGT5OTz4oc//KE+/PBD5efnKxgM6sUXX9TDDz8c9z2kipQMkmP9ZuFjnZuRkTF+3rGO46s5OYvD/e1vf9Ntt92mn/zkJzrttNMcXnVqcnIWL7zwgj766CPdeuutcVxx6nJqFl9++aUsy1JdXZ3WrVunN998UwsXLtR1112nzz77LL6bSBFOPi9GRka0YMECvfHGG3rrrbf0/e9/XzfeeKP2798fxx2kjpQMEju/Wdjj8SgSicRcFolExs871nF8NSdnMaaxsVErVqzQDTfcoJtuuikOq05NTs3i448/1oYNG7Rhwwa53e74LjpFOTWL9PR0SdKKFSsUCAQ0c+ZMXXPNNTr11FP1+uuvx3EHqcOpWUSjUd16660qLS3VySefrFmzZunee+/Vzp07tWXLlvhuIkWkZJDY+c3C2dnZ6unpibksHA4rEAiM39bhx6PRqD755JMJb9vhyJycxcjIiO677z5t2LBBv/nNb7RixYr4byCFODWLV155RV9++aUuv/xyzZ07VzfccIMkae7cuWppaYn/RlKAU7PIysrSnDlzdODAgZjjIyMj8Vt8inFqFkNDQ+rv74+ZxYwZM+RyucbDEcdg+kMs8XL11Vdbt99+uzUwMDD+qemampoJ54XDYSsnJ8fatGnT+Kemc3JyrI8//tiyLMt67rnnrIsvvtj6xz/+Mf5dNqFQyDpw4MB0bylpOTWLyspK65JLLrE+++yz6d5CynBqFodra2vjQ61T4NQsHnnkEaugoMD6+9//bkWjUeupp56ygsEg32Vjg1OzuPrqq63y8nKrt7fXikQiVnV1tfW9733PGhwcnO4tJaWUDZIvvvjCuvnmm6158+ZZ+fn5VnV1tXXw4EHLsiwrGAxaL7zwwvi5b7zxhrVo0SIrGAxaRUVFMd/JMTo6aj355JPW/PnzrWAwaC1btuyI/1HG0Tkxi76+PuvMM8+0zj77bCsYDMb8Ofz6+GpOPS8OR5BMjVOzGBkZsZ588knrsssus4LBoFVaWmp1dHRM+36SmVOz+OKLL6yKigrrwgsvtObNm2ddd911vF7YwG/7BQAAxqXkZ0gAAEByIUgAAIBxBAkAADCOIAEAAMYRJAAAwDiCBAAAGEeQAAAA4wgSAABgHEECAACMI0gAAIBxBAkAADCOIAEAAMb9X1TDArfuyp81AAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Para verlo mejor haremos un histograma\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.hist(percent_nan_row)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.172818Z",
     "start_time": "2023-11-04T14:59:40.141127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId     0.00000\n",
      "Survived        0.00000\n",
      "Pclass          0.00000\n",
      "SibSp           0.00000\n",
      "Parch           0.00000\n",
      "Fare            0.00000\n",
      "Sex_male        0.00000\n",
      "Embarked_C      0.00000\n",
      "Embarked_Q      0.00000\n",
      "Embarked_S      0.00000\n",
      "Age            19.86532\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Nans por porcentage de columna\n",
    "percent_nan_column = df_encoded.isna().sum().sort_values()/len(df_encoded)*100.\n",
    "\n",
    "print(percent_nan_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos en el print anterior que la mayoria de nans de la tabla se ven ubicados en la columna \"Age\", por lo que tenemos que analizar como vamos a tratar este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.354755Z",
     "start_time": "2023-11-04T14:59:40.154299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass        Age  SibSp  Parch     Fare  \\\n",
      "0            1.0       0.0     3.0  22.000000    1.0    0.0   7.2500   \n",
      "1            2.0       1.0     1.0  38.000000    1.0    0.0  71.2833   \n",
      "2            3.0       1.0     3.0  26.000000    0.0    0.0   7.9250   \n",
      "3            4.0       1.0     1.0  35.000000    1.0    0.0  53.1000   \n",
      "4            5.0       0.0     3.0  35.000000    0.0    0.0   8.0500   \n",
      "..           ...       ...     ...        ...    ...    ...      ...   \n",
      "886        887.0       0.0     2.0  27.000000    0.0    0.0  13.0000   \n",
      "887        888.0       1.0     1.0  19.000000    0.0    0.0  30.0000   \n",
      "888        889.0       0.0     3.0  29.699118    1.0    2.0  23.4500   \n",
      "889        890.0       1.0     1.0  26.000000    0.0    0.0  30.0000   \n",
      "890        891.0       0.0     3.0  32.000000    0.0    0.0   7.7500   \n",
      "\n",
      "     Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0         1.0         0.0         0.0         1.0  \n",
      "1         0.0         1.0         0.0         0.0  \n",
      "2         0.0         0.0         0.0         1.0  \n",
      "3         0.0         0.0         0.0         1.0  \n",
      "4         1.0         0.0         0.0         1.0  \n",
      "..        ...         ...         ...         ...  \n",
      "886       1.0         0.0         0.0         1.0  \n",
      "887       0.0         0.0         0.0         1.0  \n",
      "888       0.0         0.0         0.0         1.0  \n",
      "889       1.0         1.0         0.0         0.0  \n",
      "890       1.0         0.0         1.0         0.0  \n",
      "\n",
      "[891 rows x 11 columns]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.NaN ,strategy='mean')\n",
    "df_nans = imp.fit_transform(df_encoded)\n",
    "\n",
    "# Reconstruir DataFrame con los nombres de las columnas\n",
    "df_nans = pd.DataFrame(df_nans, columns=df_encoded.columns)\n",
    "\n",
    "print(df_nans)\n",
    "print(df_nans.isna().sum().sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar el problema de los nans, hemos usado una tecnica llamada 'Univariate' (univariado), la cual se basa en rellenar los valores nans con una variable predefinida.\n",
    "En nuestro caso hemos rellenado los nans con la media de cada columna, para no poner un valor predefinido que pueda facilitar el entrenamiento de la máquina.\n",
    "Para ver si esta solución es correcta vamos a calcular la correlación de la columna 'Age', que es la que tiene mas Nans, con 'Survived' para ver si la correlación se mantiene o no varia demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.362266Z",
     "start_time": "2023-11-04T14:59:40.355746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación antes de quitar los nans: -0.07722109457217755 \n",
      "Correlación despues de quitar los nans: -0.06980851528714307\n"
     ]
    }
   ],
   "source": [
    "corr_age_without_nans = df_nans['Survived'].corr(df_nans['Age'])\n",
    "print(f\"Correlación antes de quitar los nans: {corr['Age']} \\nCorrelación despues de quitar los nans: {corr_age_without_nans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la correlación se mantiene muy parecida por lo que podemos decir que la manera en la que hemos rellenado los nans es correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya no tenemos nans en nuestra base de datos / tabla, podemos aplicar la normalización de los valores de esta como haciamos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.362498Z",
     "start_time": "2023-11-04T14:59:40.359762Z"
    }
   },
   "outputs": [],
   "source": [
    "### Función para volver a crear la tabla correctamente.\n",
    "def rebuild_data_frame(df_new, df_original):\n",
    "    return pd.DataFrame(df_new, columns=df_original.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores que queremos normalizar son los de las columnas: 'Age' y 'Fare', ya que son los mas grandes y los que pueden hacer que confundir a la estadística.\n",
    "La siguiente función se encarga de normalizar las columnas que pasemos por parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.371994Z",
     "start_time": "2023-11-04T14:59:40.365486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass       Age  SibSp  Parch      Fare  Sex_male  \\\n",
      "0          1.0       0.0     3.0  0.271174    1.0    0.0  0.014151       1.0   \n",
      "1          2.0       1.0     1.0  0.472229    1.0    0.0  0.139136       0.0   \n",
      "2          3.0       1.0     3.0  0.321438    0.0    0.0  0.015469       0.0   \n",
      "3          4.0       1.0     1.0  0.434531    1.0    0.0  0.103644       0.0   \n",
      "4          5.0       0.0     3.0  0.434531    0.0    0.0  0.015713       1.0   \n",
      "\n",
      "   Embarked_C  Embarked_Q  Embarked_S  \n",
      "0         0.0         0.0         1.0  \n",
      "1         1.0         0.0         0.0  \n",
      "2         0.0         0.0         1.0  \n",
      "3         0.0         0.0         1.0  \n",
      "4         0.0         0.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "### Función para normalizar columnas ###\n",
    "def normalize_column(df):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    normalized_data = min_max.fit_transform(df[['Age', 'Fare']])\n",
    "    df[['Age', 'Fare']] = normalized_data\n",
    "    return df\n",
    "\n",
    "df = normalize_column(df_nans)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Preguntar la diferencia entre normalizar a traves de columnas y filas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos a continuación, ejecutando el codigo anterior, ya hemos normalizado los valores de las columnas 'Age' y 'Fare', haciendo que esten entre los valores 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 4\n",
    "¿Tenemos datos categoricos y cual sería la mejor codificación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.431041Z",
     "start_time": "2023-11-04T14:59:40.373414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass       Age  SibSp  Parch      Fare  Sex_male  \\\n",
      "0          1.0       0.0     3.0  0.271174    1.0    0.0  0.014151       1.0   \n",
      "1          2.0       1.0     1.0  0.472229    1.0    0.0  0.139136       0.0   \n",
      "2          3.0       1.0     3.0  0.321438    0.0    0.0  0.015469       0.0   \n",
      "3          4.0       1.0     1.0  0.434531    1.0    0.0  0.103644       0.0   \n",
      "4          5.0       0.0     3.0  0.434531    0.0    0.0  0.015713       1.0   \n",
      "\n",
      "   Embarked_C  Embarked_Q  Embarked_S  \n",
      "0         0.0         0.0         1.0  \n",
      "1         1.0         0.0         0.0  \n",
      "2         0.0         0.0         1.0  \n",
      "3         0.0         0.0         1.0  \n",
      "4         0.0         0.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos no disponemos de variables categoricas puesto que anteriormente, para calcular la correlación entre las diferentes variables, hemos convertido estas variables categoricas a valores numericos usando una codificación one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 5\n",
    "¿Podriamos sacar algun atributo extra de las categorias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacar atributosextra de las categorias significa crear nuevas características a partir de los datos existentes. Esto puede ayudar a mejorar el rendimiento del modelo.\n",
    "En nuestro contexto podríamos crear una nueva categoria como 'Rango de Edad', que indique si el usuario es: 'niño', 'adolescente', 'aduto' o 'anciano'. Esto nos interesaría hacerlo con una variable que esté bastante correlacionada con la variable 'Survived' como, 'Fare' y 'Sex_male'.\n",
    "En 'Sex_male' no interesa extrar atributos extra ya que por si sola ya es suficientemente útil. En canvio en 'Fare' nos podría interesar intentar extrar algun atributo extra para mejorar el modelo.\n",
    "\n",
    "Finalmente optamos por no extraer ninguna atributo extra ya que nuestra base de datos al ser suficiente pequeña no es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendriamos que usar PCA? Que beneficios nos proporcionaría?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis), es una técnica para analizar grandes conjuntos de datos, que contienen un alto numero de dimensiones/características por observación, aumentando la interpretabilidad de los datos al tiempo que se consigue preservar la maxima cantidad de información y permite la visualización de datos multidimensionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de PCA en general ayudaría a:\n",
    "- Reducir la dimensión de nuestros datos.\n",
    "- Reducir la complejidad computacional del modelo, haciendo que nuestro modelo se ejecute mas rápido.\n",
    "- Eliminar ruido y comprimir datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicamos los beneficios de PCA en general, puesto que en nuestro modelo no es necesario aplicarlo. Esto se debe a que no tenemos un gran número de variables en nuestra tabla y ya disponemos de las variables clave que están relacionadas con nuestra variable objetiva 'Survived'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 7\n",
    "¿Necesitariamos aplicar alguna tecnica de selección de features? ¿Lo en veis necesario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que las variables, 'Sex_male' y 'Pclass' son las que tienen una correlación más fuerte con nuestra variable objetivo 'Survived', por lo que a primera vista podríamos pensar que no hace falta aplicar una técnica de selección de features. La contraparte de esta afirmación, es que la correlación solo mide las relaciones lineales entre variables, por lo que es posible que existan relaciones no lineales entre nuestras variables y la variable objetivo. También hay que recalcar que puede ser que exista una combinación de características que nos dé más información que estas dos variables solas.\n",
    "\n",
    "Por lo que, una vez que hemos visto esto, podríamos pensar en aplicar, por ejemplo, RFE, eliminación recursiva de características, PCA...\n",
    "\n",
    "Aunque no nos es necesario puesto que disponemos de muy pocas variables y la dificultat de calcular la correlación no lineas es muy elevada para nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 8\n",
    "Se pueden aplicar PlynominalFeatures para mejorar la clasificación? En que modelos tiene sentido hacerlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynominalFeatures es util en modelos lineales cuando los datos no son lineales. Se agregan dimensiones para convertir el problema no lineal en lineal. Por lo que de esta manera conseguimos captar relaciones no lineales.\n",
    "\n",
    "Consideramos que no es necesario, puesto que si lo usaramos aumentariamos en gran medida el numero de variables de nuestro modelo, lo que nos llevaria a un sobreajuste y un mayor tiempo de entrenamiento de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axc-wn2rCl7T"
   },
   "source": [
    "### 3. Metric selection (1.5 punts)\n",
    "En aquest apartat ens centrarem en les mètriques de classificació ([documentació](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)). Per a fer-ho, entreneu una regressio logistica (no cal separar train-test) i a partir d'aquesta generarem una serie de funcions per analitzar els nostres resultats . Aquestes funcions ens serviran mes endevant. Caldra tambe triar la metrica que farem servir despres per triar el millor model.\n",
    "\n",
    "**Preguntes:**\n",
    "* A teoria, hem vist el resultat d'aplicar el `accuracy_score` sobre dades no balancejades. Podrieu explicar i justificar quina de les següents mètriques será la més adient pel vostre problema? `accuracy_score`, `f1_score` o `average_precision_score`?\n",
    "* Abans de comencar a entrenar models, genereu una suite de funcions per poder analitzar graficament com esta anant el vostre model. Mostreu la Precisió-Recall Curve i la ROC Curve. Quina és més rellevant pel vostre dataset? Expliqueu amb les vostres paraules, la diferencia entre una i altre [Pista](https://stats.stackexchange.com/questions/338826/auprc-vs-auc-roc)\n",
    "* Què mostra [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)? Quina métrica us fixareu per tal de optimitzar-ne la classificació pel vostre cas?\n",
    "\n",
    "Nota: Fixeu-vos que en aquest apartat NO ES VALOREN ELS RESULTATS. L'unic que es valora es l'eleccio de la metrica de classificacio aixi com saber quin tipus de grafiques fer per analitzar els resultats. Abans de solucionar un problema cal tenir molt clar la metrica d'error que es fara servir, i es una decisio que cal pendre previa a entrenar models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8xe5r78Cl7T"
   },
   "source": [
    "### 4. Model Selection amb validacio creuada (4 punts)\n",
    "\n",
    "Fent servir la metrica trobada en l'apartat anterior, en aquest apartat caldra seleccionar una serie de models i, fent us de la validacio creuada, seleccionar el millor model amb els seus respectius millors hyperparametres que haurem buscat fent hyperparameter search.\n",
    "\n",
    "La tasca d'aquesta pràctica s'enmarca dins l'aprenentatge computacional **supervisat**. A sklearn, disposem de varies tècniques [(veure documentació)](https://scikit-learn.org/stable/supervised_learning.html). A les classes de teoria, hem vist varies tècniques, com ara logistic regression, SVM amb diferents kernels, Nearest Neighbour... i tambe coneixeu altres tecniques d'altres cursos, com els arbres de decisio. Ademes, en la classe de problemes hem donat tambe els random forest i els gradient boosting. Per aquest apartat es demana seleccionar **un minim de 4 models** (per exemple, regressio logistica, random forest, KNN, SVM).\n",
    "\n",
    "**Preguntes:**\n",
    "* Quins models heu considerat? Perque els heu seleccionat?\n",
    "* Fent servir validacio creuada, escolliu el millor model (agafant els hiperparamtres per defecte). Recordeu fer servir la metrica utilitzada en l'apartat anterior. Perque es important fer servir validacio creuada? Heu de fer servir algun [tipus de validacio creuada](https://scikit-learn.org/stable/modules/cross_validation.html) en especial?\n",
    "\n",
    "* Seleccioneu una serie d'hyperparametres a provar per cadascun dels models i realitzeu una cerca d'hyperparametres. Hi ha algun model que creieu que podeu descartar de primeres? Perque?\n",
    "\n",
    "* Mostreu els resultats en una taula on es mostri el model, els experiments realitzats i els resultats obtinguts (tant en train com en test). Podeu mostrar tambe el temps d'entrenament de cada model.\n",
    "\n",
    "* Quin tipus de K-fold heu escollit en la seleccio de models? I en la seleccio de models amb hyperparametres? Com afecta al resultat modificar el numero de k (numero de folds) al resultat?\n",
    "\n",
    "* Quines formes de buscar el millor parametre heu trobat? Són costoses computacionalment parlant? [documentació](https://scikit-learn.org/stable/modules/grid_search.html) Quina heu seleccionat?\n",
    "\n",
    "* Si disposem de recursos limitats (per exemple, un PC durant 1 hora) quin dels métodes creieu que obtindrà millor resultat final?\n",
    "\n",
    "* Existeixen altres mètodes de búsqueda més eficients ([scikit-optimize](https://scikit-optimize.github.io/stable/))?\n",
    "\n",
    "* Opcional : Feu la prova, i amb el model i el metode de crossvalidació escollit, configureu els diferents metodes de búsqueda per a que s'executin durant el mateix temps (i.e. depenent del problema, 0,5h-1 hora). Analitzeu quin ha arribat a una millor solució. (Ajuda: estimeu el temps que trigarà a fer 1 training el vostre model, i aixi trobeu el número de intents que podeu fer en cada cas.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pregunta 1\n",
    "#### ¿Que modelos hemos considerado?¿Porqué los hemos escogido?\n",
    "Hemos escogido los modelos: Regresión Logística, Random Forest, KNN y SVM.\n",
    "Hemos escogido cada un por las ventajas que nos aportan:\n",
    "* Regresión Logística:\n",
    "    * Es un algoritmo simple y eficiente que nos permite manejar variables numericas y categoricas. Proporcionándonos probabilidades de los resultados a obtener que pueden ser utiles a la toma de decisiones.  \n",
    "* Random Forest:\n",
    "    * Es uno de los algoritmos de aprendizaje mas precisos que existen. Aunque funciona mejor cuanto mas grande sea nuestro conjunto de datos, trabajar con este algoritmo nos permitirá probar este modelo bastante preciso.\n",
    "* KNN (K-Nearest Neighbors): \n",
    "    * Es un algoritmo sencillo que no requiere de demasiado entrenamiento. Es un algoritmo de aprendizaje supervisado y simple que se utiliza para resolver problemas de clasificación y regresión.\n",
    "* SVM (Support Vector Machine):\n",
    "    * Es un conjunto de algoritmos de aprendizaje supervisados que se centran en los problemas de clasificación y regresión."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx-3b7v2TwJ3"
   },
   "source": [
    "### 5.Analisi Final (1.5 punt)\n",
    "\n",
    "Un cop seleccionat el millor model amb els millors hiperparamtres, caldra fer un report final amb els resultats obtinguts.\n",
    "\n",
    "Preguntes:\n",
    "* Mostreu les curves ROC/PR (la que hageu escollit en l'apartat 2) i interpreteu els resultats.\n",
    "\n",
    "* Analitzeu en detall les diferents metriques que trobeu adients i comenteu per sobre com podrieu fer servir aquest model en un futur. Aixo es el que es coneix com un cas d'us.\n",
    "\n",
    "* Com creieu que es podria millorar el vostre model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eJlr4sZkU0s-",
    "ExecuteTime": {
     "end_time": "2023-11-04T14:59:40.431864Z",
     "start_time": "2023-11-04T14:59:40.379055Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
